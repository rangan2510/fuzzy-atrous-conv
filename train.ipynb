{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fuzzy Atrous Convolutions for Covid-19 screening through Chest X-Rays"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-11-08T14:42:13.996771Z","iopub.status.busy":"2021-11-08T14:42:13.995766Z","iopub.status.idle":"2021-11-08T14:42:28.359121Z","shell.execute_reply":"2021-11-08T14:42:28.358363Z","shell.execute_reply.started":"2021-11-08T14:42:13.996652Z"},"trusted":true},"outputs":[],"source":["#Installs\n","!pip install -U fvcore\n","# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# Imports and Initialize\n","import os\n","import numpy as np\n","import pandas as pd\n","import cv2\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from statistics import mean\n","\n","#usrlibs\n","from modelhistory import ModelHistory\n","from howlong import HowLong\n","\n","#torch\n","import torch \n","import torch.nn as nn\n","from torch.nn import Conv2d #, ChannelShuffle\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset, DataLoader, Dataset\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","\n","torch.manual_seed(0)\n","torch.backends.cudnn.benchmark = False\n","\n","total_howlong = HowLong()\n","\n","DIM = 512 # SET IMAGE DIMENSION "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.361019Z","iopub.status.busy":"2021-11-08T14:42:28.360743Z","iopub.status.idle":"2021-11-08T14:42:28.426435Z","shell.execute_reply":"2021-11-08T14:42:28.425712Z","shell.execute_reply.started":"2021-11-08T14:42:28.360986Z"},"trusted":true},"outputs":[],"source":["# Hyper parameters\n","num_epochs = 1\n","num_classes = 2\n","batch_size = 10\n","learning_rate = 0.0005\n","\n","# Device configuration\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.428082Z","iopub.status.busy":"2021-11-08T14:42:28.427643Z","iopub.status.idle":"2021-11-08T14:42:28.525577Z","shell.execute_reply":"2021-11-08T14:42:28.524892Z","shell.execute_reply.started":"2021-11-08T14:42:28.428041Z"},"trusted":true},"outputs":[],"source":["lines = []\n","train_data=[]\n","test_data=[]\n","with open('input/covidx-cxr2/train.txt') as f:\n","    lines = f.readlines()\n","\n","count = 0\n","for line in lines:\n","    l=line.split()\n","    label=0\n","    if (l[2]=='positive'):\n","        label=1\n","    train_data.append([\"input/covidx-cxr2/train/\"+l[1],label])\n","\n","lines = []\n","with open('input/covidx-cxr2/test.txt') as f:\n","    lines = f.readlines()\n","\n","count = 0\n","for line in lines:\n","    l=line.split()\n","    label=0\n","    if (l[2]=='positive'):\n","        label=1\n","    test_data.append([\"../input/covidx-cxr2/test/\"+l[1],label])\n","\n","images_df = pd.DataFrame(data=train_data, columns=[\"images\", \"labels\"])\n","print(images_df.head(10))\n","images_df.groupby('labels').size()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.527949Z","iopub.status.busy":"2021-11-08T14:42:28.527535Z","iopub.status.idle":"2021-11-08T14:42:28.540727Z","shell.execute_reply":"2021-11-08T14:42:28.539919Z","shell.execute_reply.started":"2021-11-08T14:42:28.527912Z"},"trusted":true},"outputs":[],"source":["# handling imbalance\n","# create copies of the imbalanced class\n","positive_df = images_df[images_df['labels'] == 1]\n","frames = [images_df, positive_df,positive_df,positive_df,positive_df,positive_df]\n","images_df = pd.concat(frames)\n","images_df.groupby('labels').size()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.542607Z","iopub.status.busy":"2021-11-08T14:42:28.542171Z","iopub.status.idle":"2021-11-08T14:42:28.550387Z","shell.execute_reply":"2021-11-08T14:42:28.549595Z","shell.execute_reply.started":"2021-11-08T14:42:28.542571Z"},"trusted":true},"outputs":[],"source":["test = pd.DataFrame(data=test_data, columns=[\"images\", \"labels\"])\n","test.groupby('labels').size()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.552283Z","iopub.status.busy":"2021-11-08T14:42:28.551799Z","iopub.status.idle":"2021-11-08T14:42:28.581629Z","shell.execute_reply":"2021-11-08T14:42:28.580893Z","shell.execute_reply.started":"2021-11-08T14:42:28.552247Z"},"trusted":true},"outputs":[],"source":["train, val = train_test_split(images_df, stratify=images_df.labels, test_size=0.015)\n","len(train),  len(val), len(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.583159Z","iopub.status.busy":"2021-11-08T14:42:28.582787Z","iopub.status.idle":"2021-11-08T14:42:28.589776Z","shell.execute_reply":"2021-11-08T14:42:28.588905Z","shell.execute_reply.started":"2021-11-08T14:42:28.583122Z"},"trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, df_data,transform=None):\n","        super().__init__()\n","        self.df = df_data.values\n","        \n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        img_path,label = self.df[index]\n","        \n","        image = cv2.imread(img_path)\n","        image = cv2.resize(image, (DIM,DIM))\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.591837Z","iopub.status.busy":"2021-11-08T14:42:28.59148Z","iopub.status.idle":"2021-11-08T14:42:28.599065Z","shell.execute_reply":"2021-11-08T14:42:28.598253Z","shell.execute_reply.started":"2021-11-08T14:42:28.591802Z"},"trusted":true},"outputs":[],"source":["hist = ModelHistory()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.601261Z","iopub.status.busy":"2021-11-08T14:42:28.600646Z","iopub.status.idle":"2021-11-08T14:42:28.616997Z","shell.execute_reply":"2021-11-08T14:42:28.616012Z","shell.execute_reply.started":"2021-11-08T14:42:28.601222Z"},"trusted":true},"outputs":[],"source":["trans_train = transforms.Compose([transforms.ToPILImage(),\n","                                  transforms.Pad(64, padding_mode='reflect'),\n","                                  transforms.RandomHorizontalFlip(), \n","                                  transforms.RandomVerticalFlip(),\n","                                  transforms.RandomRotation(20), \n","                                  transforms.Resize(DIM, interpolation = 2),\n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n","\n","trans_valid = transforms.Compose([transforms.ToPILImage(),                    \n","                                  transforms.Pad(64, padding_mode='reflect'),\n","                                  transforms.Resize(DIM, interpolation = 2),\n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n","\n","dataset_train = MyDataset(df_data=train, transform=trans_train)\n","dataset_valid = MyDataset(df_data=val,transform=trans_valid)\n","dataset_test = MyDataset(df_data=test,transform=trans_valid)\n","\n","loader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n","loader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)\n","loader_test = DataLoader(dataset = dataset_test, batch_size=batch_size//2, shuffle=False, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:28.620205Z","iopub.status.busy":"2021-11-08T14:42:28.619721Z","iopub.status.idle":"2021-11-08T14:42:28.626434Z","shell.execute_reply":"2021-11-08T14:42:28.625471Z","shell.execute_reply.started":"2021-11-08T14:42:28.620173Z"},"trusted":true},"outputs":[],"source":["class FocalLoss(nn.modules.loss._WeightedLoss):\n","    def __init__(self, weight=None, gamma=2,reduction='mean'):\n","        super(FocalLoss, self).__init__(weight,reduction=reduction)\n","        self.gamma = gamma\n","        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n","\n","    def forward(self, input, target):\n","\n","        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n","        return focal_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:47:26.425867Z","iopub.status.busy":"2021-11-08T14:47:26.425176Z","iopub.status.idle":"2021-11-08T14:47:26.459432Z","shell.execute_reply":"2021-11-08T14:47:26.45851Z","shell.execute_reply.started":"2021-11-08T14:47:26.425791Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.nn.modules.utils import _single, _pair, _triple\n","import math\n","\n","class CustomConv(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels,kernel_size=3, dilation=2, padding=0, stride=2, bias=True, mu=0.1):\n","        super(CustomConv,self).__init__()\n","        device = \"cpu\"\n","        if(torch.cuda.is_available()):\n","            device = 'cuda'\n","            \n","        self.kernel_size=_pair(kernel_size)\n","        self.out_channels=out_channels\n","        self.dilation=_pair(dilation)\n","        self.padding=_pair(padding)\n","        self.stride=_pair(stride)\n","        self.in_channels=in_channels\n","        self.mu=mu\n","        self.mu_=(1-mu)/3\n","        self.bias1=torch.nn.Parameter(torch.Tensor(out_channels))\n","        self.device = device\n","        \n","        #self.bias1=self.bias1.to(device)\n","        mu_=self.mu_\n","        self.calculated_kernel_size=self.dilation[0]*(self.kernel_size[0]-1)+1\n","        self.weight=torch.nn.Parameter(torch.Tensor(self.out_channels,self.in_channels,self.kernel_size[0],self.kernel_size[1]))\n","\n","        self.fuz=self.mask_dial(self.kernel_size[0],self.dilation[0],self.mu)\n","        self.fuz[self.fuz==1] = 0\n","        self.fuz=self.fuz.unsqueeze(0)#.unsqueeze(0).unsqueeze(0)\n","        \n","        temp=self.fuz\n","        for i in range(1,self.in_channels):\n","            temp=torch.cat((temp,self.fuz))\n","        temp=temp.unsqueeze(0)\n","        temp1=temp\n","        for i in range(1,self.out_channels):\n","            temp1=torch.cat((temp1,temp))\n","        self.fuz=temp1\n","        if(bias):\n","            self.bias=torch.nn.Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameters(\"bias\",None)\n","        self.fuz=self.fuz.to(self.device)\n","        self.reset_parameters()\n","    \n","    def mask_dial(self,kernel_size,dilation,mu):\n","        dilation-=1\n","        mid=[0 for i in range(dilation)]\n","        lim=(dilation//2) if (dilation%2==0) else ((dilation//2)+1)\n","        diff=(1-mu)/lim\n","        filter1=[]\n","        for i in range(lim):\n","            mid[i]=1-(i+1)*diff\n","            mid[dilation-1-i]=1-(i+1)*diff\n","        for i in range(2*kernel_size-1):\n","            if(i%2==0):\n","                filter1=filter1+[0]\n","            else:\n","                filter1=filter1+mid\n","        filter2=[[0 for i in range(dilation+2)] for j in range(dilation)]\n","        for i in range(lim):\n","            for j in range(i+2):\n","                filter2[i][j]=mid[i]\n","                filter2[i][dilation+1-j]=mid[i]\n","                filter2[dilation-i-1][j]=mid[i]\n","                filter2[dilation-i-1][dilation+1-j]=mid[i]\n","            for j in range(i+1,lim):\n","                filter2[i][j+1]=mid[j]\n","                filter2[i][dilation-j]=mid[j]\n","                filter2[dilation-i-1][j+1]=mid[j]\n","                filter2[dilation-i-1][dilation-j]=mid[j]\n","        filter3=[x[1:] for x in filter2]\n","        for i in range(kernel_size-2):\n","            for j in range(len(filter2)):\n","                filter2[j]+=filter3[j]\n","        result=[]\n","        for i in range(2*kernel_size-1):\n","            if(i%2==0):\n","                result=result+[filter1]\n","            else:\n","                result=result+filter2\n","                result=[0 for i in range(2*kernel_size-1)]\n","        result=[]\n","        for i in range(2*kernel_size-1):\n","            if(i%2==0):\n","                result=result+[filter1]\n","            else:\n","                result=result+filter2\n","        result=torch.Tensor(result)\n","        return result\n","\n","    \n","    def reset_parameters(self):\n","        stdv=math.sqrt(6./((self.in_channels*(self.kernel_size[0]**2))+(self.out_channels*(self.kernel_size[0]**2))))\n","        self.weight.data.uniform_(-stdv,stdv)\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv,stdv)\n","        self.bias1.data.uniform_(-stdv,stdv)\n","    \n","    \n","    def forward(self, input_):\n","\n","        hout = ((input_.shape[2]+2*self.padding[0]-self.calculated_kernel_size)//self.stride[0])+1\n","        wout = ((input_.shape[3]+2*self.padding[1]-self.calculated_kernel_size)//self.stride[1])+1\n","               \n","        weight_kernel = F.unfold(input_,kernel_size=self.kernel_size,dilation=self.dilation,stride=self.stride).to(self.device)\n","        fuzzy_kernel = F.unfold(input_,kernel_size=self.calculated_kernel_size,dilation=1,stride=self.stride).to(self.device)\n","        \n","        convolvedOutput = (fuzzy_kernel.transpose(1,2).matmul((((self.fuz.permute(1,2,3,0))*self.bias1).permute(3,0,1,2)).flatten(1).transpose(0,1))).transpose(1,2)+(weight_kernel.transpose(1,2).matmul((self.weight).flatten(1).transpose(0,1))).transpose(1,2)\n","        convolutionReconstruction=convolvedOutput.view(input_.shape[0],self.out_channels,hout,wout)\n","        return convolutionReconstruction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:47:31.343897Z","iopub.status.busy":"2021-11-08T14:47:31.343339Z","iopub.status.idle":"2021-11-08T14:47:31.364978Z","shell.execute_reply":"2021-11-08T14:47:31.364248Z","shell.execute_reply.started":"2021-11-08T14:47:31.343841Z"},"trusted":true},"outputs":[],"source":["class TOFU(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv1 = Conv2d( in_channels, 16, kernel_size=3, dilation=1, padding=1, padding_mode='replicate' )\n","        self.conv2 = Conv2d( 16,32, kernel_size=3, dilation=3, padding=3, padding_mode='replicate')\n","        self.conv3 = Conv2d(16+32,48, kernel_size=3, dilation=5, padding=5, padding_mode='replicate')\n","        \n","        self.compress = Conv2d(16+32+48,out_channels, kernel_size=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        \n","    def forward(self, x):\n","        x1 = F.relu(self.conv1(x))\n","        x2 = F.relu(self.conv2(x1))\n","        x3 = F.relu(self.conv3(torch.cat([x1,x2],1)))\n","        x4 = F.relu(self.compress(torch.cat([x1,x2,x3],1)))\n","        x4 = self.bn(x4)\n","        return x4\n","        \n","class MOFU(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.fconv1 = CustomConv(in_channels,16,kernel_size=3, stride=2,dilation=2, mu=0.1)\n","        self.fconv2 = CustomConv(16,32,kernel_size=3, stride=3,dilation=3, mu=0.3)\n","        self.fconv3 = CustomConv(32,out_channels,kernel_size=3, stride=5,dilation=5, mu=0.5)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.fconv1(x))\n","        x = F.relu(self.fconv2(x))\n","        x = F.relu(self.fconv3(x))\n","        x = self.bn(x)\n","        return x\n","        \n","\n","class Net(nn.Module): \n","    def __init__(self, num_classes): \n","        super().__init__()\n","        self.tofu = TOFU(3,32)\n","        self.mofu = MOFU(32,32)\n","        self.fc = nn.Linear(32*15*15, num_classes) \n","        self.gradients = None\n","        \n","    def forward(self, x):\n","        x = self.tofu(x)\n","        x = x[:,torch.randperm(x.size()[1])]\n","        x = self.mofu(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x\n","\n","net = Net(num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:47:35.683329Z","iopub.status.busy":"2021-11-08T14:47:35.682912Z","iopub.status.idle":"2021-11-08T14:47:35.694995Z","shell.execute_reply":"2021-11-08T14:47:35.693922Z","shell.execute_reply.started":"2021-11-08T14:47:35.683287Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","from torchvision import models\n","model = net.to(device)\n","pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Total trainable params:\", pytorch_total_params)\n","pytorch_total_params = sum(p.numel() for p in model.parameters())\n","print(\"All params:\", pytorch_total_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:47:39.275578Z","iopub.status.busy":"2021-11-08T14:47:39.274694Z","iopub.status.idle":"2021-11-08T14:47:39.280744Z","shell.execute_reply":"2021-11-08T14:47:39.279887Z","shell.execute_reply.started":"2021-11-08T14:47:39.275519Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","criterion = FocalLoss()\n","optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n","scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=0.0002)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.load_state_dict(torch.load(\"/input/covid-cxr-3-layers/final_state.dct\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:42:43.555967Z","iopub.status.busy":"2021-11-08T14:42:43.5555Z","iopub.status.idle":"2021-11-08T14:42:43.567292Z","shell.execute_reply":"2021-11-08T14:42:43.566624Z","shell.execute_reply.started":"2021-11-08T14:42:43.555928Z"},"trusted":true},"outputs":[],"source":["def eval(model):\n","    model.eval()  \n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        loss = 0\n","        \n","        for images, labels in loader_valid:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            \n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            \n","            loss += criterion(outputs, labels)\n","\n","        val_loss = loss / len(loader_valid)\n","        acc = 100 * correct / total\n","\n","    return(acc, val_loss)\n","\n","def test(model):\n","    model.eval()  \n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        y_true = []\n","        y_pred = []\n","        for images, labels in loader_test:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            y_true.append(labels.cpu().detach())\n","            y_pred.append(predicted.cpu().detach())\n","            \n","        y_true = torch.cat(y_true).numpy()\n","        y_pred = torch.cat(y_pred).numpy()\n","        print(\"f1:\\t\",f1_score(y_true, y_pred, average='macro'))\n","        print(\"prec:\\t\",precision_score(y_true, y_pred, average='macro'))\n","        print(\"recall:\\t\",recall_score(y_true, y_pred, average='macro'))\n","    return(100 * correct / total)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T13:29:34.928679Z","iopub.status.busy":"2021-11-08T13:29:34.92796Z","iopub.status.idle":"2021-11-08T13:29:34.947648Z","shell.execute_reply":"2021-11-08T13:29:34.946511Z","shell.execute_reply.started":"2021-11-08T13:29:34.928635Z"},"trusted":true},"outputs":[],"source":["# Train the model\n","total_step = len(loader_train)\n","train_howlong = HowLong()\n","\n","for epoch in range(num_epochs):\n","    train_losses = []\n","    print(\"Epoch \", epoch+1,\" started...\")\n","    epoch_howlong = HowLong()\n","    model.train()\n","    \n","    for i, (images, labels) in enumerate(loader_train):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","                \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step(epoch)\n","        train_losses.append(loss.item())\n","\n","        if (i+1) % 100 == 0:\n","            print ('  Epoch [{:2d}/{:2d}] \\t Step [{:3d}/{:3d}] \\t Train Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))                \n","            \n","    acc, val_loss = eval(model)\n","    print ('  Val Loss: {:.4f} \\t Val Acc: {:.2f}'.format(val_loss.item(), acc))    \n","    \n","    hist.add_metric(train_loss=mean(train_losses), valid_loss=val_loss.item(), valid_acc=acc)\n","    hist.save_checkpoint(model.state_dict(), val_loss.item())\n","    \n","    epoch_howlong.since_last()\n","    \n","\n","train_howlong.since_start()\n","hist.save_checkpoint(model.state_dict(), val_loss.item(), name=\"final_state\", unconditional=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:48:01.285727Z","iopub.status.busy":"2021-11-08T14:48:01.28548Z","iopub.status.idle":"2021-11-08T14:48:34.665338Z","shell.execute_reply":"2021-11-08T14:48:34.664537Z","shell.execute_reply.started":"2021-11-08T14:48:01.2857Z"},"trusted":true},"outputs":[],"source":["print(\"Evaluating model...\")\n","\n","acc = test(model)\n","print(acc)\n","\n","total_howlong.since_start()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T13:30:14.402949Z","iopub.status.busy":"2021-11-08T13:30:14.402393Z","iopub.status.idle":"2021-11-08T13:30:21.738186Z","shell.execute_reply":"2021-11-08T13:30:21.73621Z","shell.execute_reply.started":"2021-11-08T13:30:14.402901Z"},"trusted":true},"outputs":[],"source":["from fvcore.nn import FlopCountAnalysis, flop_count_table\n","images, labels =  next(iter(loader_test))\n","image = images[0].unsqueeze(0)\n","image = image.to(device)\n","flops = FlopCountAnalysis(model, image)\n","print(flops.total())\n","print(flop_count_table(flops))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T13:30:21.740763Z","iopub.status.busy":"2021-11-08T13:30:21.740188Z","iopub.status.idle":"2021-11-08T13:30:21.756743Z","shell.execute_reply":"2021-11-08T13:30:21.755278Z","shell.execute_reply.started":"2021-11-08T13:30:21.740716Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(hist.get_metrics())"]},{"cell_type":"markdown","metadata":{},"source":["# Run the following cells to obtain gradcam image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T13:30:21.759543Z","iopub.status.busy":"2021-11-08T13:30:21.758837Z","iopub.status.idle":"2021-11-08T13:30:21.781542Z","shell.execute_reply":"2021-11-08T13:30:21.780601Z","shell.execute_reply.started":"2021-11-08T13:30:21.759497Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","\n","class Net(nn.Module): \n","    def __init__(self, num_classes): \n","        super().__init__()\n","        self.tofu = TOFU(3,32)\n","        self.mofu = MOFU(32,32)\n","        self.fc = nn.Linear(32*15*15, num_classes) \n","        self.gradients = None\n","    def activations_hook(self, grad):\n","        self.gradients = grad\n","    def get_activations_gradient(self):\n","        return self.gradients\n","    \n","    # method for the activation exctraction\n","    def get_activations(self, x):\n","        return self.features_conv(x)\n","    def forward(self, x):\n","        x = self.tofu(x)\n","        x = self.mofu.fconv1(x)\n","        x = self.mofu.fconv2(x)\n","        x = self.mofu.fconv3(x)\n","        h = x.register_hook(self.activations_hook)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x\n","\n","net = Net(num_classes)\n","model=net.to(device)\n","criterion = FocalLoss()\n","optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T14:47:52.794877Z","iopub.status.busy":"2021-11-08T14:47:52.794159Z","iopub.status.idle":"2021-11-08T14:47:52.809204Z","shell.execute_reply":"2021-11-08T14:47:52.808404Z","shell.execute_reply.started":"2021-11-08T14:47:52.794816Z"},"trusted":true},"outputs":[],"source":["#to use latest weights after model is trained, replace the following uncommented code by the commented code\n","model.load_state_dict(torch.load(\"../input/covid-cxr-3-layers/final_state.dct\"))\n","#model.load_state_dict(torch.load(\"./final_state.dct\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T13:30:21.833851Z","iopub.status.busy":"2021-11-08T13:30:21.83327Z","iopub.status.idle":"2021-11-08T13:30:22.845546Z","shell.execute_reply":"2021-11-08T13:30:22.844287Z","shell.execute_reply.started":"2021-11-08T13:30:21.833805Z"},"trusted":true},"outputs":[],"source":["#creating gradcam image\n","img_path=\"../input/covidx-cxr2/train/000001-1.jpg\"\n","image = cv2.imread(img_path)\n","image = cv2.resize(image, (DIM,DIM))\n","trans = transforms.Compose([transforms.ToPILImage(),                    \n","                                  transforms.Pad(64, padding_mode='reflect'),\n","                                  transforms.Resize(DIM, interpolation = 2),\n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n","img = trans(image).to(device)\n","pred = model(img.unsqueeze(0))#.argmax(dim=1)\n","pred.shape\n","pred[:,torch.argmax(pred)].backward()\n","gradients = net.get_activations_gradient()\n","gradients.shape\n","pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n","activations = net.tofu(img.unsqueeze(0))\n","activations = net.mofu.fconv1(activations)\n","activations = net.mofu.fconv2(activations)\n","activations = net.mofu.fconv3(activations).detach()\n","activations.shape\n","for i in range(32):\n","    activations[:, i, :, :] *= pooled_gradients[i]\n","heatmap = torch.mean(activations, dim=1).squeeze()\n","heatmap = np.maximum(heatmap.cpu(), 0)\n","heatmap /= torch.max(heatmap)\n","plt.matshow(heatmap.squeeze())\n","img=img.permute(1,2,0)\n","heatmap = cv2.resize(heatmap.numpy(), (img.shape[0], img.shape[1]))\n","heatmap = np.uint8(255 * heatmap)\n","heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n","superimposed_img = heatmap * 0.1 + img.cpu().numpy()\n","cv2.imwrite('./map.jpg', superimposed_img)\n","plt.imshow( heatmap * 0.002 + img.cpu().numpy()  )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-08T13:30:22.847095Z","iopub.status.busy":"2021-11-08T13:30:22.846826Z","iopub.status.idle":"2021-11-08T13:30:23.021575Z","shell.execute_reply":"2021-11-08T13:30:23.020664Z","shell.execute_reply.started":"2021-11-08T13:30:22.847054Z"},"trusted":true},"outputs":[],"source":["#save gradcam and original image\n","x=heatmap * 0.002 + img.cpu().numpy()\n","y=np.transpose(x, axes=[2,0,1])\n","y=torch.tensor(y)\n","from torchvision.utils import save_image\n","save_image(y, 'gradcam1.png')\n","img=img.permute(2, 0, 1)\n","save_image(img, 'original1.png')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
